{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blonsbrough/First-Repository/blob/main/PHYS_555_02_DeepLearning_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please copy the notebook and fill. Once filled, upload it to the course Brightspace.\n",
        "\n",
        "---\n",
        "If you worked with other students, provide name(s) here:"
      ],
      "metadata": {
        "id": "LsC1CeKDnfUh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o40y8HxDYE3G"
      },
      "source": [
        "# ECG Anomaly Detection with Autoencoders\n",
        "\n",
        "This homework will consists of a guided notebook where you will have to fill-in some blanks and comment. The notebook explores how we can detect abnormal electrocardiograms (ECG) with deep learning. It is likely not the most performant or best approach, but it will show the main concepts. You do not have to obtain a very performant model.\n",
        "\n",
        "\n",
        "\n",
        "### Data\n",
        "The dataset contains samples of 5,000 ECG with 140 timesteps. Each sequence corresponds to a single heartbeat from a single patient with congestive heart failure. If you are not familiar with ECG, you may read up a bit on it, e.g. on [wikipedia](https://en.wikipedia.org/wiki/Electrocardiography).\n",
        "\n",
        "\n",
        "Here the dataset has simply been labelled with two classes: normal and abnormal.\n",
        "We will use the labels to split the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start with boiler plate code."
      ],
      "metadata": {
        "id": "iila6eyHmN_y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RY_N3gOmfDi"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation\n",
        "\n",
        "The original data can be found on PhysioNet and is described [here](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000). It was all prepared to be one single CSV file and can be found on [tensorflow tutorial](https://www.tensorflow.org/tutorials/generative/autoencoder#third_example_anomaly_detection).\n",
        "We follow the same pre-processing steps and some of the evaluation procedures, with the slight differences, and with PyTorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ebpt30MRoo4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
        "raw_data = dataframe.values\n",
        "dataframe.head()\n",
        "\n",
        "# The last element contains the labels\n",
        "labels = raw_data[:, -1]\n",
        "\n",
        "# The other data points are the ECG data\n",
        "data = raw_data[:, 0:-1]\n",
        "\n",
        "n_features = data.shape[1]\n",
        "\n",
        "# split full data into training and test set\n",
        "xtrain_data, test_data, ytrain_labels, test_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=21)\n",
        "\n",
        "#  split a validation set from the training set\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    xtrain_data, ytrain_labels, test_size=0.2, random_state=22)\n",
        "\n",
        "\n",
        "print(train_data.shape, val_data.shape, test_data.shape)"
      ],
      "metadata": {
        "id": "KlHWZvCODIVu",
        "outputId": "fbb1d024-c128-4a1f-89e4-12a5cf58cd2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3198, 140) (800, 140) (1000, 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data are numpy arrays. Use scikit-learn `MinMaxScaling` to scale appropriately the training, validation and test sets."
      ],
      "metadata": {
        "id": "m7pCUeiovAGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = # your code\n",
        "train_data = # your code\n",
        "val_data = # your code\n",
        "test_data = # your code\n",
        "\n",
        "train_labels = train_labels.astype(bool)\n",
        "val_labels = val_labels.astype(bool)\n",
        "test_labels = test_labels.astype(bool)\n",
        "\n",
        "normal_train_data = train_data[train_labels]\n",
        "normal_val_data = val_data[val_labels]\n",
        "normal_test_data = test_data[test_labels]\n",
        "\n",
        "anomalous_train_data = train_data[~train_labels]\n",
        "anomalous_val_data = val_data[~val_labels]\n",
        "anomalous_test_data = test_data[~test_labels]"
      ],
      "metadata": {
        "id": "HjelQbF_u50Q",
        "outputId": "fe55f0bd-bf99-4b36-f400-1bc3ec3addfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-36c0537cb973>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    scaler = # your code\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot a few samples or normal and abnormal ECGs."
      ],
      "metadata": {
        "id": "Mk1pJi4dvYra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(n_features), normal_train_data[0])\n",
        "plt.title(\"Normal\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FarCta-6_Njg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(n_features), anomalous_train_data[0])\n",
        "plt.title(\"Anomalous\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i06JdoSr_Tm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a `Dataloader`  for each of the normal training, validation, and test set. It could be as simple as you want to be."
      ],
      "metadata": {
        "id": "08VVDjoAyJER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code\n",
        "# train_loader...\n",
        "# val_loader...\n",
        "# test_loader..."
      ],
      "metadata": {
        "id": "TnJJCx_uyHvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All good. Let now build a first simple fully connected autoencoder. You are free to use any number of hidden layers and neurons, and the type of activation functions.\n",
        "\n",
        "You could for example use encoding with number of inputs as: 140->32->16->8 and symmetric decoding, e.g. 8->16->32->140.\n",
        "\n",
        "Remember to be consistent in the last activation with the choice of the loss function later on."
      ],
      "metadata": {
        "id": "wP12vtW6zQmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGAutoencoder(nn.Module):\n",
        "\n",
        "  def __init__(self, n_features, z_dim):\n",
        "    super(ECGAutoencoder, self).__init__()\n",
        "\n",
        "    self.encoder = # your code\n",
        "    #\n",
        "    #\n",
        "    self.decoder = # your code\n",
        "    #\n",
        "    #\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "Xf4NpegCDRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ENnubQdnJN"
      },
      "source": [
        "### Training\n",
        "\n",
        "Let's write a helper function for our training process, that will take an autoencoder neural network model, both a training and a validation `DataLoader` and will loop over `n_epochs`.\n",
        "\n",
        "Make sure to return the trained model, and arrays of the losses at each epoch to allow post-training plots."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, n_epochs=150):\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "  criterion = # choose a loss function\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    model = model.train()\n",
        "\n",
        "    train_batch_losses = []\n",
        "\n",
        "    for data in train_loader:\n",
        "      # your code\n",
        "      #\n",
        "      #\n",
        "      #\n",
        "\n",
        "      train_batch_losses.append(loss.item())\n",
        "\n",
        "    val_batch_losses = []\n",
        "    model = model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data in val_loader:\n",
        "          # your code\n",
        "          #\n",
        "          #\n",
        "          #\n",
        "\n",
        "          val_losses.append(loss.item())\n",
        "\n",
        "    train_loss = np.mean(train_batch_losses)\n",
        "    val_loss = np.mean(val_batch_losses)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Losses Train = {train_loss:.4f} Valid = {val_loss:.4f}\")\n",
        "\n",
        "  # plot losses\n",
        "  ax = plt.figure().gca()\n",
        "  ax.plot(train_losses)\n",
        "  ax.plot(val_losses)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'val'])\n",
        "  plt.title('Loss monitoring')\n",
        "  plt.show()\n",
        "\n",
        "  # return trained model and loss arrays.\n",
        "  return model.eval()"
      ],
      "metadata": {
        "id": "aD3KWC6jZRpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train an `ECGAutoencoder` with a latent dimension of 8, plot the loss history and save the model."
      ],
      "metadata": {
        "id": "VI7nsu1f_AZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ECGAutoencoder(n_features, 8)\n",
        "model = train_model(model, train_loader, val_loader)\n",
        "\n",
        "torch.save(model, 'ecg_autoencoder.pt')"
      ],
      "metadata": {
        "id": "QueaFEkjgJXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2IGZFBEVJjq"
      },
      "source": [
        "\n",
        "#### Comments\n",
        "\n",
        "Add a couple lines of comments on the training procedure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwLujPFXT054"
      },
      "source": [
        "## Evaluating the outputs\n",
        "\n",
        "The trained model can be used to reconstruct the ECGs.\n",
        "Let's plot some of the reconstructed ECG segments, both from the normal and anomalous.\n",
        "\n",
        "Create a prediction function that takes a trained model and a dataset, and returns arrays of the predictions and the losses.\n",
        "\n",
        "We will use these to plot the reconstruction and histograms of the losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, data):\n",
        "\n",
        "  preds, losses = [], []\n",
        "  criterion = # your code. reuse the same loss as in your training procedure.\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model = model.eval()\n",
        "    #  your code\n",
        "\n",
        "  # should return a tuple of an array of the predictions and an array the corresponding loss values\n",
        "  return preds, losses"
      ],
      "metadata": {
        "id": "0FLQ1QA_gv-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBRWRk6WWdNC"
      },
      "source": [
        "def plot_reconstruction(data, model, title, ax):\n",
        "  recon, losses = predict(model, data)\n",
        "  ax.plot(data, label='original')\n",
        "  ax.plot(recon[0], label='reconstructed')\n",
        "  ax.set_title(f'{title}: loss = {np.around(losses[0], 3)}')\n",
        "  ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnN1K63BYomX"
      },
      "source": [
        "n_ex = 4\n",
        "fig, axs = plt.subplots(nrows=2, ncols=n_ex, sharex=True, sharey=True, figsize=(22, 8))\n",
        "\n",
        "for col, data in enumerate(normal_test_data[:n_ex]):\n",
        "  plot_reconstruction(data, model, title='Normal', ax=axs[0, col])\n",
        "\n",
        "for col, data in enumerate(anomalous_test_data[:n_ex]):\n",
        "  plot_reconstruction(data, model, title='Anomalous', ax=axs[1, col])\n",
        "\n",
        "fig.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional autoencoder\n",
        "\n",
        "Now copy the`ECGAutoencoder` network definition, and modify it with a Convolutional Autoencoder. You are fairly free to build your network, here is a proposal:\n",
        "\n",
        "- `self.encoder`: two blocks, each consisting of a one-dimensional convolutional layer, a ReLU activation, and a one-dimentional Batch normalisation. You are free to choose kernel size, strides and padding. You can add 2 fully-connected layers, to produce a compressed latent space.\n",
        "\n",
        "- `self.decoder`: the symmetric of the encoder, by using `ConvTranspose1D` instead to expand to larger layers."
      ],
      "metadata": {
        "id": "60CDEN6u1ugA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGConvAutoencoder(nn.Module):\n",
        "\n",
        "  def __init__(self, n_features, z_dim):\n",
        "    super(ECGConvAutoencoder, self).__init__()\n",
        "    self.encoder = # your code\n",
        "    self.decoder = # your code\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "AzMM8KXvtzuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the new network"
      ],
      "metadata": {
        "id": "2cWoCDcbBsax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ECGConvAutoencoder(n_features, 8)\n",
        "model = train_model(model, train_loader, val_loader)\n",
        "\n",
        "torch.save(model, 'ecg_convautoencoder.pt')"
      ],
      "metadata": {
        "id": "WQ19VnfauAdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=2, ncols=n_ex, sharex=True, sharey=True, figsize=(22, 8))\n",
        "\n",
        "for col, data in enumerate(normal_test_data[:n_ex]):\n",
        "  plot_reconstruction(data, model, title='Normal', ax=axs[0, col])\n",
        "\n",
        "for col, data in enumerate(anomalous_test_data[:n_ex]):\n",
        "  plot_reconstruction(data, model, title='Anomalous', ax=axs[1, col])\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "kqDbks41uP_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anomalous samples detection\n",
        "\n",
        "We can detect anomalies by evaluating whether the reconstruction metric (i.e. the loss for a sample) is greater than a given threshold.\n",
        "\n",
        "For example, we can take the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n",
        "\n",
        "Choose a threshold value that is one standard deviation above the mean of all the training sample losses."
      ],
      "metadata": {
        "id": "m_xnTQ5TKBzZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnNtIreDXf5"
      },
      "source": [
        "Our function goes through each example in the dataset and records the predictions and losses. Let's get the losses and have a look at them.\n",
        "\n",
        "The model can either the the convolutional or the fully-connected autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvn141SDS33P"
      },
      "source": [
        "_, train_losses = predict(model, normal_train_data)\n",
        "\n",
        "plt.hist(train_losses, bins=50)\n",
        "plt.xlabel(\"Train loss\")\n",
        "plt.ylabel(\"# of examples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = # your code"
      ],
      "metadata": {
        "id": "9pzjOdGJQsIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw2dm631T4a5"
      },
      "source": [
        "The problem is framed as a detection for a sample to be above or below the selected threshold.\n",
        "\n",
        "\n",
        "Let's see how well the model performs on normal samples of the test set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z630B5v7Fid"
      },
      "source": [
        "preds, losses = predict(model, normal_test_data)\n",
        "plt.hist(losses, bins=50)\n",
        "plt.xlabel(\"Test loss\")\n",
        "plt.ylabel(\"# of examples\")\n",
        "plt.title(\"Normal sample\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hidyhcu6zC8-"
      },
      "source": [
        "Do the same with the anomalous sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLCuS8oL7hG2"
      },
      "source": [
        "preds, losses = predict(model, anomalous_test_data)\n",
        "plt.hist(losses, bins=50)\n",
        "plt.xlabel(\"Test loss\")\n",
        "plt.ylabel(\"# of examples\")\n",
        "plt.title(\"Anomalous sample\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkgGXs4E06so"
      },
      "source": [
        "Finally, we can count the number of examples above and below the threshold similar to a classification model. Use standar metrics such as accuracy, precision and recall to evaluate your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_normal(model, data, threshold):\n",
        "  # your code\n",
        "  # returns true if loss is less than threshold\n",
        "\n",
        "def print_stats(predictions, labels):\n",
        "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
        "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
        "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
      ],
      "metadata": {
        "id": "kQcmSaVpvI8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = is_normal(model, test_data, threshold)\n",
        "print_stats(preds, test_labels)"
      ],
      "metadata": {
        "id": "BbCoDDYl3VZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra\n",
        "\n",
        "(Optional == Bonus points).\n",
        "If time permits, you can build, train and evaluate other type or architectures\n",
        "- an LSTM Autoencoder: it will take into account some time-dependence of the samples\n",
        "- a VAE."
      ],
      "metadata": {
        "id": "9mNbtCE0Cj6k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gThJBv8FDIFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}